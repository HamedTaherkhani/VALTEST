import asyncio
import openai
from typing import List
import os
import re
from dotenv import load_dotenv
from tqdm import tqdm
import concurrent.futures
load_dotenv()

# Set your OpenAI API key
openai.api_key = os.getenv('openai_key')


prompt1 = """
You are given a function implementation, function description and some test cases generated by LLMs. These test cases are invalid test cases generated by the function description. You are asked to categorize the reason why the llm generated invalid test case. Invalid test case means that it won't pass on the ground truth function implementation. To categorize the invalid test cases generated by LLMs, you must look at the function implementations and try to understand why the generated test cases are invalid. Here are some possible categories of issues that may arise, with descriptions for each:

### 1. **Misunderstanding of Function Logic**
   - **Description**: The LLM-generated test case does not match the actual function logic or expected behavior. The function may involve complex logic, edge cases, or specific conditions that the LLM did not infer from the docstring.
   - **Example**: The test case expects the function to return a result in cases where the function should actually raise an error or handle the case differently.

### 2. **Incorrect Parameter Types**
   - **Description**: The LLM-generated test case provides incorrect types for function parameters, leading to type errors or incorrect outputs. This could happen if the docstring doesn't clearly specify types, or the LLM misinterprets them.
   - **Example**: Passing a string where an integer is expected, or passing a list instead of a single value.

### 3. **Incorrect Return Value Expectations**
   - **Description**: The LLM expects a return value that doesn’t match the actual return type or structure of the function. This could happen if the function returns complex objects like lists, tuples, or dictionaries, but the LLM infers a different structure.
   - **Example**: Expecting a single integer return, but the function returns a tuple or a dictionary.

### 4. **Edge Case Misinterpretation**
   - **Description**: The test case fails because the LLM didn’t properly handle an edge case. The function implementation has specific behavior for unusual or extreme input values that the LLM did not infer.
   - **Example**: A test case that passes negative values to a function that is only designed to handle positive numbers.

### 5. **Off-by-One or Range Errors**
   - **Description**: The test case expects results that are slightly off due to an off-by-one or range-related misunderstanding. These errors often occur when functions deal with sequences, loops, or boundaries.
   - **Example**: Expecting an index to start at 1 when it actually starts at 0, leading to incorrect test case outputs.

### 6. **Incorrect Assumptions About Input-Output Mapping**
   - **Description**: The LLM may infer a simple mapping between input and output based on the docstring, but the actual function involves more complex transformations or checks that were not apparent.
   - **Example**: Assuming a function is a straightforward arithmetic operation when it’s actually performing data validation before processing the inputs.

### 7. **Failure to Handle Exceptions**
   - **Description**: The test case doesn’t account for exceptions that should be raised by the function. The function might raise specific exceptions based on certain inputs, but the LLM test cases do not check for or expect those exceptions. For example Division by Zero Error.
   - **Example**: A function is designed to raise a `ValueError` when invalid inputs are passed, but the LLM generates test cases expecting a normal return value instead of handling the exception.

### 8. **Incorrect Assumptions About Side Effects**
   - **Description**: The LLM might assume that the function has or does not have side effects (e.g., modifying input objects, changing global state), leading to invalid test case expectations.
   - **Example**: The function modifies a list in place, but the test case expects a new list to be returned, or vice versa.

### 9. **Ignoring Required Pre-Conditions or Invariants**
   - **Description**: Some functions may require certain pre-conditions (e.g., sorted inputs, non-empty lists) to work properly, which are not mentioned explicitly in the docstring. LLM-generated test cases may fail to account for these conditions.
   - **Example**: Passing an unsorted list to a function that assumes the input is already sorted.

### 10. **Ambiguities or Incompleteness in the Docstring**
   - **Description**: If the docstring is ambiguous, incomplete, or lacks details, the LLM might generate test cases based on incorrect assumptions, leading to assertion errors when run on the function implementation.
   - **Example**: A docstring that doesn’t specify behavior for certain input ranges or edge cases, causing the LLM to make an incorrect guess.

### 11. **Overfitting to Common Patterns**
   - **Description**: The LLM may overfit to common coding patterns or generic cases, missing out on the specificities of the function at hand. It might generate test cases that are valid in a general sense but invalid for the specific function implementation.
   - **Example**: Assuming a function works similarly to a well-known algorithm, but the function implementation uses a custom or less common approach.

### 12. **Assuming Pure Functions When They Aren't**
   - **Description**: The LLM may assume the function is pure (i.e., its output only depends on its inputs and has no side effects) when in reality, the function might involve state changes, I/O, or randomness.
   - **Example**: A function that uses randomization or external data sources, but the LLM-generated test case assumes a deterministic output.

### 13. **Rounding Expectations**
   - **Description**: The invalid test cases assume that the result should be rounded to the nearest integer or a specific decimal place, which is not explicitly handled in the function implementation.

Please categorize each test case and map each test case to a number. put the results in between *****. Please review all categories and find the category which is the closest one.
The final output should be something like this.:
*****
1. Ambiguities or Incompleteness in the Docstring (Category 10)
2. Ambiguities or Incompleteness in the Docstring (Category 10)
3. Ambiguities or Incompleteness in the Docstring (Category 10)
*****
---
"""

prompt2 = "You are given a function implementation, function description and some test cases generated by LLMs. These test cases are invalid test cases generated by the function description. You are asked to categorize the reason why the llm generated invalid test case. Invalid test case means that it won't pass on the ground truth function implementation. To categorize the invalid test cases generated by LLMs, you must look at the function implementations and try to understand why the generated test cases are invalid."

import ast


def run_function_from_assertion(function_code: str, assertion_code: str):
    # Execute the function code
    exec(function_code, globals())

    # Parse the assertion to extract the function call and the expected output
    tree = ast.parse(assertion_code, mode='exec')
    assert_node = tree.body[0]  # Get the assertion node

    if isinstance(assert_node, ast.Assert):
        # Extract the function call and the expected output
        function_call = assert_node.test.left  # The function call
        expected_output_node = assert_node.test.comparators[0]  # The expected value

        # Get the function name
        function_name = function_call.func.id  # The name of the function being called

        # Extract all arguments for the function call
        try:
            input_values = [ast.literal_eval(arg) for arg in function_call.args]  # List of arguments
        except Exception as e:
            raise Exception(e)
        # Extract the actual expected output value from the AST node
        expected_output = ast.literal_eval(expected_output_node)
        try:
            # Dynamically call the function with multiple arguments
            actual_output = eval(f"{function_name}(*{input_values})")
        except Exception as e:
            return e.__str__(), expected_output
        return actual_output, expected_output
    else:
        raise ValueError("No valid assertion found in the assertion code.")


class Category:
    def __init__(self, index: int, name: str, description: str):
        self.index = index
        self.name = name
        self.description = description

    def __repr__(self):
        return f"Category {self.index}: {self.name} - {self.description}"


# Define all category objects
categories = [
    Category(1, "Misunderstanding of Function Logic",
             "The LLM-generated test case does not match the actual function logic or expected behavior."),
    Category(2, "Incorrect Parameter Types",
             "The LLM-generated test case provides incorrect types for function parameters."),
    Category(3, "Incorrect Return Value Expectations",
             "The LLM expects a return value that doesn’t match the actual return type or structure of the function."),
    Category(4, "Edge Case Misinterpretation",
             "The test case fails because the LLM didn’t properly handle an edge case."),
    Category(5, "Off-by-One or Range Errors",
             "The test case expects results that are slightly off due to an off-by-one or range-related misunderstanding."),
    Category(6, "Incorrect Assumptions About Input-Output Mapping",
             "The LLM infers a simple input-output mapping but the function involves complex transformations."),
    Category(7, "Failure to Handle Exceptions",
             "The test case doesn’t account for exceptions that should be raised by the function."),
    Category(8, "Incorrect Assumptions About Side Effects",
             "The LLM assumes the function has or doesn’t have side effects."),
    Category(9, "Ignoring Required Pre-Conditions or Invariants",
             "The function requires certain pre-conditions that the LLM-generated test case fails to account for."),
    Category(10, "Ambiguities or Incompleteness in the Docstring",
             "Ambiguous or incomplete docstring causes LLM to make incorrect assumptions."),
    Category(11, "Overfitting to Common Patterns", "The LLM may overfit to common coding patterns or generic cases."),
    Category(12, "Assuming Pure Functions When They Aren't",
             "The LLM assumes the function is pure when it has state changes, I/O, or randomness."),
    Category(13, "Rounding Expectations",
             "The invalid test cases assume that the result should be rounded to the nearest integer or specific decimal place.")
]


def find_category_number(text):
    # Use a regular expression to find the number after 'Category'
    match = re.search(r'Category\s*(\d+)', text)
    if match:
        return int(match.group(1))
    else:
        return None


def map_response_to_category(response_text):
    """
    Map the response from GPT-4 to the predefined categories.
    """
    # Extract the categorized lines between the ***** markers
    categorized_text = get_category_from_response(response_text)
    categorized_lines = categorized_text.split('\n')
    # Attempt to map each line to a category by matching descriptions
    mapped_categories = []
    for line in categorized_lines:
        cate = find_category_number(line)
        matched_category = None
        for category in categories:
            if category.index == cate:
                matched_category = category
                break
        if matched_category:
            mapped_categories.append(matched_category)
        else:
            mapped_categories.append(Category(-1, "Unknown", "Could not map to any predefined category"))

    return mapped_categories


def get_category_from_response(response_text):
    start_marker = "*****"
    end_marker = "*****"

    try:
        # Find the part of the text that contains the categorization
        start_idx = response_text.index(start_marker) + len(start_marker)
        end_idx = response_text.index(end_marker, start_idx)
        categorized_text = response_text[start_idx:end_idx].strip()
        return categorized_text
    except ValueError:
        # In case the markers aren't found, return an empty string
        return ""


def categorize_invalid_tests(functions: List['Function'], client, model="o1-preview"):
    max_workers = 5  # Limit to 5 concurrent threads
    results = []

    def process_function(function):
        # Build the prompt
        prompt = prompt1  # Ensure 'prompt1' is defined
        invalid_tests = [tc.text for tc in function.testcases if not tc.is_valid]
        if len(invalid_tests) == 0:
            return None
        prompt += '\n' + "function description:" + '\n' + function.prompt + '\n' + 'generated tests:\n'
        for invalid_test in invalid_tests:
            prompt += invalid_test + ' \n'
            try:
                actual_output, expected_output = run_function_from_assertion(function.solution, invalid_test)
            except Exception as e:
                continue
            prompt += f'\nBy running the test cases, we can see that the actual output is: {actual_output} but the expected output is: {expected_output}\n'
        prompt += '\n' + "function implementation:\n" + function.solution

        # Send the prompt to GPT-4 and get the response
        response = client.chat.completions.create(
            model=model,
            messages=[
                # {"role": "system", "content": "You are an expert in Python testing."},
                {"role": "user", "content": prompt}
            ],
            # max_tokens=2000,  # Adjust token limit based on your needs
            # temperature=0
        )

        response_text = response.choices[0].message.content
        mapped_categories = map_response_to_category(response_text)
        print([m.index for m in mapped_categories])
        function_result = {
            'function_prompt': function.prompt,
            'function_implementation': function.solution,
            'test_case_results': []
        }

        for idx, gg in enumerate(invalid_tests):
            # Assign the mapped category for each test case based on the response
            category = mapped_categories[idx] if idx < len(mapped_categories) else Category(-1, "Unknown", "Could not map to any category")
            function_result['test_case_results'].append({
                'test_case_text': gg,
                'category_index': category.index,
                'category_name': category.name,
            })
        return function_result

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(process_function, function): function for function in functions}
        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):
            result = future.result()
            if result is not None:
                results.append(result)
    return results


if __name__ == '__main__':
    import sys

    from openai import OpenAI
    client = OpenAI(api_key=os.getenv('openai_key'))
    dataset = 'HumanEval'
    llm = 'gpt-4o'
    from typing import List
    from log_probs import Function
    import pickle

    with open(f'filtered_testcases/{dataset}_{llm}.pkl', 'rb') as f:
        functions: List[Function] = pickle.load(f)
    functions = functions
    # Ensure that 'functions' and 'prompt1' are defined before calling the function
    categorized_results = categorize_invalid_tests(functions, client)
    number_of_invalid_tests = 0
    for f in functions:
        for tt in f.testcases:
            if not tt.is_valid:
                number_of_invalid_tests += 1

    # Count the number of test cases per category
    category_counts = {}
    for result in categorized_results:
        for test_case in result['test_case_results']:
            category_name = test_case['category_name']
            category_counts[category_name] = category_counts.get(category_name, 0) + 1

    # Print the counts
    print(f'Total number of invalid tests: {number_of_invalid_tests}')
    print("\nTest Case Counts per Category:")
    for category_name, count in category_counts.items():
        print(f"  {category_name}: {count}")

    # Save the entire results into a pickle file
    with open('categorized_results.pkl', 'wb') as f:
        pickle.dump(categorized_results, f)
