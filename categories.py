import asyncio
import openai
from typing import List
import os
import re
from dotenv import load_dotenv
from tqdm import tqdm
import concurrent.futures
load_dotenv()

# Set your OpenAI API key
openai.api_key = os.getenv('openai_key')


prompt1 = """
You are given a function implementation, function description and some test cases generated by LLMs. These test cases are invalid test cases generated by the function description. You are asked to categorize the reason why the llm generated invalid test case. Invalid test case means that it won't pass on the ground truth function implementation. To categorize the invalid test cases generated by LLMs, you must look at the function implementations and try to understand why the generated test cases are invalid. Here are some possible categories of issues that may arise, with descriptions for each:

### 1. **Misunderstanding of Function Logic**
   - **Description**: The LLM-generated test case does not match the actual function logic or expected behavior. The function may involve complex logic, edge cases, or specific conditions that the LLM did not infer from the docstring.
   - **Example**: The test case expects the function to return a result in cases where the function should actually raise an error or handle the case differently.

### 2. **Edge Case Misinterpretation**
   - **Description**: The test case fails because the LLM didn’t properly handle an edge case. The function implementation has specific behavior for unusual or extreme input values that the LLM did not infer.
   - **Example**: A test case that passes negative values to a function that is only designed to handle positive numbers.

### 3. **Off-by-One or Range Errors**
   - **Description**: The test case expects results that are slightly off due to an off-by-one or range-related misunderstanding. These errors often occur when functions deal with sequences, loops, or boundaries.
   - **Example**: Expecting an index to start at 1 when it actually starts at 0, leading to incorrect test case outputs.

### 4. **Incorrect Input-Output Mapping**
   - **Description**: The LLM may infer a simple mapping between input and output based on the docstring, but the actual function involves more complex transformations or checks that were not apparent.
   - **Example**: Assuming a function is a straightforward arithmetic operation when it’s actually performing data validation before processing the inputs.

### 5. **Ambiguities or Incompleteness in the Docstring**
   - **Description**: If the docstring is ambiguous, incomplete, or lacks details, the LLM might generate test cases based on incorrect assumptions, leading to assertion errors when run on the function implementation.
   - **Example**: A docstring that doesn’t specify behavior for certain input ranges or edge cases, causing the LLM to make an incorrect guess.

### 6. **Rounding Expectations**
   - **Description**: The invalid test cases assume that the result should be rounded to the nearest integer or a specific decimal place, which is not explicitly handled in the function implementation.

Please categorize each test case and map each test case to a number. put the results in between *****. Please review all categories and find the category which is the closest one.
The final output should be something like this.:
*****
1. Ambiguities or Incompleteness in the Docstring (Category 5)
2. Ambiguities or Incompleteness in the Docstring (Category 5)
3. Ambiguities or Incompleteness in the Docstring (Category 5)
*****
---
"""

prompt2 = "You are given a function implementation, function description and some test cases generated by LLMs. These test cases are invalid test cases generated by the function description. You are asked to categorize the reason why the llm generated invalid test case. Invalid test case means that it won't pass on the ground truth function implementation. To categorize the invalid test cases generated by LLMs, you must look at the function implementations and try to understand why the generated test cases are invalid."

import ast


def run_function_from_assertion(function_code: str, assertion_code: str):
    # Execute the function code
    exec(function_code, globals())

    # Parse the assertion to extract the function call and the expected output
    tree = ast.parse(assertion_code, mode='exec')
    assert_node = tree.body[0]  # Get the assertion node

    if isinstance(assert_node, ast.Assert):
        # Extract the function call and the expected output
        function_call = assert_node.test.left  # The function call
        expected_output_node = assert_node.test.comparators[0]  # The expected value

        # Get the function name
        function_name = function_call.func.id  # The name of the function being called

        # Extract all arguments for the function call
        try:
            input_values = [ast.literal_eval(arg) for arg in function_call.args]  # List of arguments
        except Exception as e:
            raise Exception(e)
        # Extract the actual expected output value from the AST node
        expected_output = ast.literal_eval(expected_output_node)
        try:
            # Dynamically call the function with multiple arguments
            actual_output = eval(f"{function_name}(*{input_values})")
        except Exception as e:
            return e.__str__(), expected_output
        return actual_output, expected_output
    else:
        raise ValueError("No valid assertion found in the assertion code.")


class Category:
    def __init__(self, index: int, name: str, description: str):
        self.index = index
        self.name = name
        self.description = description

    def __repr__(self):
        return f"Category {self.index}: {self.name} - {self.description}"


# Define all category objects
categories = [
    Category(1, "Misunderstanding of Function Logic",
             "The LLM-generated test case does not match the actual function logic or expected behavior."),
    Category(2, "Edge Case Misinterpretation",
             "The test case fails because the LLM didn’t properly handle an edge case."),
    Category(3, "Off-by-One or Range Errors",
             "The test case expects results that are slightly off due to an off-by-one or range-related misunderstanding."),
    Category(4, "Incorrect Input-Output Mapping",
             "The LLM infers a simple input-output mapping but the function involves complex transformations."),
    Category(5, "Ambiguities or Incompleteness in the Docstring",
             "Ambiguous or incomplete docstring causes LLM to make incorrect assumptions."),
    Category(6, "Rounding Expectations",
             "The invalid test cases assume that the result should be rounded to the nearest integer or specific decimal place.")
]


def find_category_number(text):
    # Use a regular expression to find the number after 'Category'
    match = re.search(r'Category\s*(\d+)', text)
    if match:
        return int(match.group(1))
    else:
        return None


def map_response_to_category(response_text):
    """
    Map the response from GPT-4 to the predefined categories.
    """
    # Extract the categorized lines between the ***** markers
    categorized_text = get_category_from_response(response_text)
    categorized_lines = categorized_text.split('\n')
    # Attempt to map each line to a category by matching descriptions
    mapped_categories = []
    for line in categorized_lines:
        cate = find_category_number(line)
        matched_category = None
        for category in categories:
            if category.index == cate:
                matched_category = category
                break
        if matched_category:
            mapped_categories.append(matched_category)
        else:
            mapped_categories.append(Category(-1, "Unknown", "Could not map to any predefined category"))

    return mapped_categories


def get_category_from_response(response_text):
    start_marker = "*****"
    end_marker = "*****"

    try:
        # Find the part of the text that contains the categorization
        start_idx = response_text.index(start_marker) + len(start_marker)
        end_idx = response_text.index(end_marker, start_idx)
        categorized_text = response_text[start_idx:end_idx].strip()
        return categorized_text
    except ValueError:
        # In case the markers aren't found, return an empty string
        return ""


def categorize_invalid_tests(functions: List['Function'], client, model="o1-preview"):
    max_workers = 5  # Limit to 5 concurrent threads
    results = []

    def process_function(function):
        # Build the prompt
        prompt = prompt1  # Ensure 'prompt1' is defined
        invalid_tests = [tc for tc in function.testcases if not tc.is_valid]
        if len(invalid_tests) == 0:
            return None
        prompt += '\n' + "function description:" + '\n' + function.prompt + '\n' + 'generated tests:\n'
        for invalid_test in invalid_tests:
            prompt += invalid_test.text + ' \n'
            try:
                actual_output, expected_output = run_function_from_assertion(function.solution, invalid_test.text)
            except Exception as e:
                continue
            prompt += f'\nBy running the test cases, we can see that the actual output is: {actual_output} but the expected output is: {expected_output}\n'
        prompt += '\n' + "function implementation:\n" + function.solution

        # Send the prompt to GPT-4 and get the response
        response = client.chat.completions.create(
            model=model,
            messages=[
                # {"role": "system", "content": "You are an expert in Python testing."},
                {"role": "user", "content": prompt}
            ],
            # max_tokens=2000,  # Adjust token limit based on your needs
            # temperature=0
        )

        response_text = response.choices[0].message.content
        mapped_categories = map_response_to_category(response_text)
        print([m.index for m in mapped_categories])
        function_result = {
            'function_prompt': function.prompt,
            'function_implementation': function.solution,
            'test_case_results': []
        }

        for idx, gg in enumerate(invalid_tests):
            # Assign the mapped category for each test case based on the response
            category = mapped_categories[idx] if idx < len(mapped_categories) else Category(-1, "Unknown", "Could not map to any category")
            function_result['test_case_results'].append({
                'test_case_text': gg.text,
                'category_index': category.index,
                'category_name': category.name,
                # 'predicted': gg.prediction_is_valid
            })
        return function_result

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(process_function, function): function for function in functions}
        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):
            result = future.result()
            if result is not None:
                results.append(result)
    return results


if __name__ == '__main__':
    import sys

    from openai import OpenAI
    client = OpenAI(api_key=os.getenv('openai_key'))
    dataset = 'HumanEval'
    llm = 'GeminiPro'
    from typing import List
    from log_probs import Function, TestCase, RawLogProbs
    import pickle
    from function_executor import run_test_cases
    with open(f'unfiltered_testcases/{dataset}_{llm}.pkl', 'rb') as f:
        functions: List[RawLogProbs] = pickle.load(f)
    functions = functions
    funcs: List[Function] = []
    for idx,func in tqdm(enumerate(functions)):
        # print(idx)
        ## HE
        if idx in (39,76):
            continue
        is_passed_list = run_test_cases(func.solution, func.testcases, timeout=5)
        is_passed_list = [1 if gg else 0 for gg in is_passed_list]
        tests:List[TestCase] =[]
        # print(is_passed_list)
        for index,item in enumerate(is_passed_list):
            # print(item)
            tests.append(TestCase(text=func.testcases[index],input_logprobs=[],output_logprobs=[],second_input_logprobs=[],second_output_logprobs=[],is_valid=is_passed_list[index]))
        funcs.append(Function(solution=func.solution, testcases=tests, prompt=func.prompt))
    functions = funcs
    # Ensure that 'functions' and 'prompt1' are defined before calling the function
    categorized_results = categorize_invalid_tests(functions, client)
    number_of_invalid_tests = 0
    for f in functions:
        for tt in f.testcases:
            if not tt.is_valid:
                number_of_invalid_tests += 1

    # Count the number of test cases per category
    category_counts = {}
    for result in categorized_results:
        for test_case in result['test_case_results']:
            category_name = test_case['category_name']
            category_counts[category_name] = category_counts.get(category_name, 0) + 1

    # Print the counts
    print(f'Total number of invalid tests: {number_of_invalid_tests}')
    print("\nTest Case Counts per Category:")
    for category_name, count in category_counts.items():
        print(f"  {category_name}: {count}")

    # Save the entire results into a pickle file
    with open('output/categorized_results_gemini.pkl', 'wb') as f:
        pickle.dump(categorized_results, f)
